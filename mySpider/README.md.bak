# Scrapy 爬虫框架案例

制作 Scrapy 爬虫 一共需要4步：
1. 新建项目 (scrapy startproject xxx)：新建一个新的爬虫项目

2. 明确目标 （编写items.py）：明确你想要抓取的目标

3. 制作爬虫 （spiders/xxspider.py）：制作爬虫开始爬取网页
创建一个名为itcast的爬虫，并指定爬取域的范围：
scrapy genspider itcast "itcast.cn"

文件说明：
name = "" ：这个爬虫的识别名称，必须是唯一的，在不同的爬虫必须定义不同的名字。
allow_domains = [] 是搜索的域名范围，也就是爬虫的约束区域，规定爬虫只爬取这个域名下的网页，不存在的URL会被忽略。
start_urls = () ：爬取的URL元祖/列表。爬虫从这里开始抓取数据，所以，第一次下载的数据将会从这些urls开始。其他子URL将会从这些起始URL中继承性生成。
parse(self, response) ：解析的方法，每个初始URL完成下载后将被调用，调用的时候传入从每一个URL传回的Response对象来作为唯一参数，主要作用如下：
负责解析返回的网页数据(response.body)，提取结构化数据(生成item)

4. 存储内容 （pipelines.py）：设计管道存储爬取内容

主要文件的作用：
scrapy.cfg ：项目的配置文件
mySpider/ ：项目的Python模块，将会从这里引用代码
mySpider/items.py ：项目的目标文件
mySpider/pipelines.py ：项目的管道文件
mySpider/settings.py ：项目的设置文件
mySpider/spiders/ ：存储爬虫代码目录

scrapy保存信息的最简单的方法主要有四种，-o 输出指定格式的文件，命令如下：
scrapy crawl itcast -o teachers.json
scrapy crawl itcast -o teachers.jsonl
scrapy crawl itcast -o teachers.csv
scrapy crawl itcast -o teachers.xml
